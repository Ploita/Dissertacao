{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_envs_reward(enviroment):\n",
    "    vetor_de_listas = [vec.episode_returns for vec in enviroment.envs]\n",
    "    max_len = max(len(lista) for lista in vetor_de_listas)\n",
    "\n",
    "    # Preencha as listas mais curtas com NaN para manter o comprimento\n",
    "    vetor_preenchido = [lista + [np.nan] * (max_len - len(lista)) for lista in vetor_de_listas]\n",
    "\n",
    "    # Converta para um array numpy para facilitar o manuseio\n",
    "    array_preenchido = np.array(vetor_preenchido)\n",
    "\n",
    "    # Calcule a média e o desvio padrão para cada ponto\n",
    "    media = np.nanmean(array_preenchido, axis=0)\n",
    "    desvio_padrao = np.nanstd(array_preenchido, axis=0)\n",
    "\n",
    "    # Crie o gráfico\n",
    "    x = np.arange(1, max_len + 1)\n",
    "    plt.plot(x, media, label='Média')\n",
    "    plt.fill_between(x, media - desvio_padrao, media + desvio_padrao, alpha=0.3, label='Desvio Padrão')\n",
    "\n",
    "    plt.xlabel('Episódios')\n",
    "    plt.ylabel('Recompensa média')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
    "\n",
    "num_cpu = 8\n",
    "env_id = \"CartPole-v1\"\n",
    "\n",
    "search_env = make_vec_env(env_id, n_envs=num_cpu, vec_env_cls=DummyVecEnv)\n",
    "vai_treinar_em_ep_sim = StopTrainingOnMaxEpisodes(max_episodes= 1000, verbose= 0)\n",
    "\n",
    "# Se essa não for a prova definitiva da minha incompetência, eu n sei o que vai ser\n",
    "hparams = {'learning_rate': 0.0023,\n",
    " 'gamma': 0.99,\n",
    " 'batch_size': 64,\n",
    " 'buffer_size': 10000,\n",
    " 'train_freq': 256,\n",
    " 'target_update_interval': 10,\n",
    " 'learning_starts': 1000,\n",
    " 'gradient_steps': 128,\n",
    " 'policy_kwargs': dict(net_arch = [256, 256]),\n",
    "# 'max_grad_norm': 1,\n",
    " 'exploration_initial_eps': 1,\n",
    " 'exploration_final_eps': 0.04,\n",
    " 'exploration_fraction': 0.16}\n",
    "\n",
    "teste = DQN('MlpPolicy', search_env, verbose=0, **hparams)\n",
    "teste.learn(total_timesteps=int(1e10), callback= vai_treinar_em_ep_sim, progress_bar=True)\n",
    "\n",
    "eval_env = make_vec_env(env_id, n_envs=1, vec_env_cls=DummyVecEnv)\n",
    "mean_reward, _ = evaluate_policy(teste, search_env, n_eval_episodes=1000 * 1, return_episode_rewards= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "env_name = 'CartPole-v1'\n",
    "search_env = Monitor(gym.make(env_name))#, render_mode = 'human'))\n",
    "\n",
    "# Função de avaliação\n",
    "def evaluate(trial):\n",
    "    model_params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-3), #, log=True),\n",
    "        #'gamma': trial.suggest_categorical('gamma', [0.99]),\n",
    "        #'batch_size': trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128]),\n",
    "        #'buffer_size': trial.suggest_categorical('buffer_size', [10000, 50000, 100000]),\n",
    "        #'train_freq': trial.suggest_categorical('train_freq', [8, 16, 128, 256]),\n",
    "        #'target_update_interval': trial.suggest_categorical('target_update_interval', [100]),\n",
    "        #'learning_starts': trial.suggest_categorical('learning_starts', [1000])\n",
    "        #'gradient_steps': trial.suggest_categorical('gradient_steps', [1, 4, 8, 16, 32, 64, 128]),\n",
    "        #'exploration_initial_eps': trial.suggest_float('exploration_initial_eps', 0.9, 1.0),\n",
    "        #'exploration_final_eps': trial.suggest_float('exploration_final_eps', 0.0, 0.3),\n",
    "        #'max_grad_norm': trial.suggest_categorical('max_grad_norm',[1.0, 10.0])\n",
    "    }\n",
    "    \n",
    "    model = DQN('MlpPolicy', search_env, verbose=0, **model_params)\n",
    "    \n",
    "    # Treinamento do modelo\n",
    "    model.learn(total_timesteps=300)  # Ajuste o número de etapas conforme necessário\n",
    "\n",
    "    # Avaliação do modelo\n",
    "    mean_reward, _ = evaluate_policy(model, search_env, n_eval_episodes=1000)\n",
    "\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "class CheckHyperparamMeanRewardThreshold:\n",
    "    def __init__(self, reward_threshold: float):\n",
    "        self.reward_threshold = reward_threshold\n",
    "\n",
    "    def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial) -> None:\n",
    "        if trial.value is not None and trial.value >= self.reward_threshold:\n",
    "            print((f'Stopping hyperparameter search because trial.value ({trial.value}) '\n",
    "                   f'hit threshold ({self.reward_threshold})'))\n",
    "            study.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# Define o estudo Optuna\n",
    "study = optuna.create_study(direction='maximize', pruner= optuna.pruners.ThresholdPruner(5, None, 0, 1))\n",
    "study.optimize(evaluate, n_trials = 10, callbacks=[CheckHyperparamMeanRewardThreshold(500.0)], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém os melhores hiperparâmetros\n",
    "best_model_params = {**study.best_params}\n",
    "\n",
    "train_env = Monitor(gym.make(env_name))\n",
    "# Criação do modelo com os melhores hiperparâmetros\n",
    "best_model = DQN('MlpPolicy', train_env, **best_model_params)\n",
    "\n",
    "# Treinamento do modelo final\n",
    "best_model.learn(total_timesteps=int(1e5), log_interval=1, progress_bar=True)\n",
    "\n",
    "eval_env = Monitor(gym.make(env_name))\n",
    "_ = evaluate_policy(best_model, eval_env, n_eval_episodes=int(1e3), return_episode_rewards = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_rewards = np.array(search_env.get_episode_rewards())\n",
    "plt.plot(train_rewards)\n",
    "plt.ylim(0, 550)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rewards = np.array(train_env.get_episode_rewards())\n",
    "plt.plot(train_rewards)\n",
    "plt.ylim(0, 550)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array(eval_env.get_episode_rewards())\n",
    "plt.plot(rewards)\n",
    "plt.ylim(0, 550)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rewards)\n",
    "plt.title('Histograma')\n",
    "plt.xlabel('Valores')\n",
    "plt.ylabel('Frequência')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = best_model.env.envs[0].episode_returns\n",
    "recompensas = a\n",
    "plt.plot(recompensas)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dissertacao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
