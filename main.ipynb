{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1', render_mode = 'human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/06 14:00:06 INFO mlflow.tracking.fluent: Experiment with name 'DQN_24_02_06' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/108688011859788956', creation_time=1707238806027, experiment_id='108688011859788956', last_update_time=1707238806027, lifecycle_stage='active', name='DQN_24_02_06', tags={}>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# connect mlflow client to the mlflow server that runs on localhost:5000\n",
    "MLFLOW_SERVER_URI = 'http://localhost:5000'\n",
    "mlflow.set_tracking_uri(str(MLFLOW_SERVER_URI))\n",
    "\n",
    "EXPERIMENT_NAME = 'DQN_24_02_07'\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-06 14:00:06,136] Using an existing study with name 'DQN_24_02_06' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "from config import OPTUNA_DB\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=EXPERIMENT_NAME,\n",
    "    direction='maximize',\n",
    "    load_if_exists=True,\n",
    "    storage=f'sqlite:///{OPTUNA_DB}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216e4504521b4f6d8de7543e946102d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67,586 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.81it/s]\n",
      "100%|██████████| 200/200 [00:35<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward mean: 317.04, std: 111.01\n",
      "Num steps mean: 317.04, std: 111.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:12<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2024-02-06 14:01:56,073] Trial 479 finished with value: 322.083 and parameters: {'learning_rate': 6.85863829888384e-05, 'discount_factor': 0.95, 'batch_size': 16, 'memory_size': 50000, 'freq_steps_train': 8, 'freq_steps_update_target': 10, 'n_steps_warm_up_memory': 1000, 'n_gradient_steps': 16, 'nn_hidden_layers': 2, 'max_grad_norm': 10, 'normalize_state': True, 'epsilon_start': 0.9, 'epsilon_end': 0.125869250878496, 'steps_epsilon_decay': 100000, 'seed': 804151958}. Best is trial 229 with value: 500.0.\n",
      "Stopping hyperparameter search because trial.value (322.083) hit threshold (50.0)\n"
     ]
    }
   ],
   "source": [
    "from optimize_hyperparameters import objective\n",
    "\n",
    "# we define a lambda function because study.optimize()\n",
    "# expect the objective function to have only 1 input\n",
    "# (trial), while our objective function hast 2 extra\n",
    "# inputs I defined to add flexibility to the script\n",
    "func = lambda trial: objective(trial,\n",
    "                               force_linear_model=False,\n",
    "                               n_episodes_to_train=200)\n",
    "\n",
    "class CheckHyperparamMeanRewardThreshold:\n",
    "    def __init__(self, reward_threshold: float):\n",
    "        self.reward_threshold = reward_threshold\n",
    "\n",
    "    def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial) -> None:\n",
    "        if trial.value is not None and trial.value >= self.reward_threshold:\n",
    "            print((f'Stopping hyperparameter search because trial.value ({trial.value}) '\n",
    "                   f'hit threshold ({self.reward_threshold})'))\n",
    "            study.stop()\n",
    "\n",
    "# Stop hyperparameter search when we hit a perfect mean reward of 500\n",
    "hyperparam_search_stop_callback = CheckHyperparamMeanRewardThreshold(50.0)\n",
    "\n",
    "study.optimize(func, n_trials=1000, callbacks=[hyperparam_search_stop_callback], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 6.465531216430013e-05, 'discount_factor': 0.9, 'batch_size': 32, 'memory_size': 100000, 'freq_steps_train': 256, 'freq_steps_update_target': 100, 'n_steps_warm_up_memory': 1000, 'n_gradient_steps': 16, 'nn_hidden_layers_index': 0, 'max_grad_norm': 1, 'normalize_state': True, 'epsilon_start': 0.9, 'epsilon_end': 0.19225553957356353, 'steps_epsilon_decay': 1000}\n",
      "Seed:  900695048\n"
     ]
    }
   ],
   "source": [
    "best_trial = study.best_trial\n",
    "\n",
    "hparams = {k: best_trial.params[k] for k in best_trial.params if k != 'seed'}\n",
    "#hparams['nn_hidden_layers'] = eval(hparams['nn_hidden_layers']) \n",
    "print(hparams)\n",
    "\n",
    "SEED = best_trial.params['seed']\n",
    "print('Seed: ', SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 6.465531216430013e-05,\n",
       " 'discount_factor': 0.9,\n",
       " 'batch_size': 32,\n",
       " 'memory_size': 100000,\n",
       " 'freq_steps_train': 256,\n",
       " 'freq_steps_update_target': 100,\n",
       " 'n_steps_warm_up_memory': 1000,\n",
       " 'n_gradient_steps': 16,\n",
       " 'nn_hidden_layers_index': 0,\n",
       " 'max_grad_norm': 1,\n",
       " 'normalize_state': True,\n",
       " 'epsilon_start': 0.9,\n",
       " 'epsilon_end': 0.19225553957356353,\n",
       " 'steps_epsilon_decay': 1000}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "QAgent.__init__() got an unexpected keyword argument 'nn_hidden_layers_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m set_seed(env, SEED)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mq_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QAgent\n\u001b[1;32m----> 5\u001b[0m agent \u001b[38;5;241m=\u001b[39m QAgent(env, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhparams)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mloops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m      8\u001b[0m train_rewards, train_steps \u001b[38;5;241m=\u001b[39m train(agent, env, n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: QAgent.__init__() got an unexpected keyword argument 'nn_hidden_layers_index'"
     ]
    }
   ],
   "source": [
    "from utils import set_seed\n",
    "set_seed(env, SEED)\n",
    "\n",
    "from q_agent import QAgent\n",
    "agent = QAgent(env, **hparams)\n",
    "\n",
    "from loops import train\n",
    "train_rewards, train_steps = train(agent, env, n_episodes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, len(train_rewards) + 1), train_rewards, label='Rewards per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Rewards per Episode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_agent import QAgent\n",
    "from config import SAVED_AGENTS_DIR\n",
    "\n",
    "# you can find the agent_id for the best run in the MLflow\n",
    "# dashboard.\n",
    "# 298 is the value in my case, but you need to check what is your\n",
    "agent_id = 6\n",
    "\n",
    "path_to_saved_model = SAVED_AGENTS_DIR / 'CartPole-v1' / str(agent_id)\n",
    "agent = QAgent.load_from_disk(env, path_to_saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loops import evaluate\n",
    "rewards, steps = evaluate(\n",
    "    agent, env,\n",
    "    n_episodes=1000,\n",
    "    epsilon=0.00\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(rewards) + 1), rewards, label='Rewards per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Rewards per Episode')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "reward_avg = np.array(rewards).mean()\n",
    "reward_std = np.array(rewards).std()\n",
    "print(f'Reward average {reward_avg:.2f}, std {reward_std:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 4))\n",
    "ax.set_title(\"Rewards\")    \n",
    "pd.Series(rewards).plot(kind='hist', bins=100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seria bom o gráfico de progresso do treino"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dissertacao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
