{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa388ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from npeet import entropy_estimators as ee\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.stats import gaussian_kde\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# --- Configurações Iniciais ---\n",
    "n_samples_global = int(1e4) # Amostras para testes com dados uniformes\n",
    "noise_scale_global = 1.5\n",
    "k_values_knn = [3, 5, 10]\n",
    "n_bins_values_binning = [10, 20, 40]\n",
    "n_jobs_values_sklearn = [1]\n",
    "n_features_to_test_uniform = [1, 3, 5] # Para o loop original com dados uniformes\n",
    "n_features_to_test_gaussian = [1, 2] # Número de features em X para o teste Gaussiano\n",
    "n_samples_gaussian_analytical = int(5e4) # Amostras para o teste Gaussiano\n",
    "base_log_manual = np.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results_list = []\n",
    "#==============================================================================\n",
    "# TESTE ANALÍTICO COM DADOS GAUSSIANOS (X MULTIVARIADO, Y UNIVARIADO)\n",
    "#==============================================================================\n",
    "print(\"\\n--- INICIANDO TESTE ANALÍTICO COM DADOS GAUSSIANOS (X MULTIVARIADO) ---\")\n",
    "\n",
    "for n_feat_x_analytical in n_features_to_test_gaussian:\n",
    "    print(f\"\\n--- Teste Gaussiano com N_Features_X = {n_feat_x_analytical} ---\")\n",
    "    dim_Y_analytical = 1\n",
    "    total_dims = n_feat_x_analytical + dim_Y_analytical\n",
    "\n",
    "    # Construir a matriz de covariância Sigma\n",
    "    # Sigma_XX: Identidade (features X_i são N(0,1) e independentes entre si)\n",
    "    sigma_xx = np.eye(n_feat_x_analytical)\n",
    "    # Sigma_YY: Variância de Y (N(0,1))\n",
    "    sigma_yy_val = 1.0\n",
    "    sigma_yy = np.array([[sigma_yy_val]])\n",
    "\n",
    "    # Sigma_XY: Covariâncias entre X_i e Y\n",
    "    # Vamos criar uma correlação modesta para cada X_i com Y, garantindo Sigma > 0\n",
    "    # Ex: cov(X_i, Y) = c / sqrt(n_feat_x_analytical) para que sum(cov^2) não exploda\n",
    "    # Para simplicidade, vamos definir uma correlação alvo para a primeira feature X1 com Y\n",
    "    # e correlações menores ou zero para as outras, garantindo SPD.\n",
    "    \n",
    "    target_overall_correlation_strength = 0.6 # Ajuste este valor\n",
    "    sigma_xy = np.zeros((n_feat_x_analytical, dim_Y_analytical))\n",
    "    \n",
    "    # Exemplo: correlação apenas com a primeira feature de X\n",
    "    # sigma_xy[0, 0] = target_overall_correlation_strength \n",
    "    # Para garantir que a soma dos quadrados das correlações seja < 1 se Sigma_XX=I, Sigma_YY=1:\n",
    "    # Distribuir a \"força de correlação\"\n",
    "    # Se n_feat_x_analytical = 1, rho = target_overall_correlation_strength\n",
    "    # Se > 1, podemos ter rho_i = target_overall_correlation_strength / sqrt(n_feat_x_analytical)\n",
    "    # ou definir manualmente para controle\n",
    "    \n",
    "    if n_feat_x_analytical == 1:\n",
    "        sigma_xy[0,0] = target_overall_correlation_strength\n",
    "    elif n_feat_x_analytical == 2:\n",
    "        sigma_xy[0,0] = 0.5 # corr(X1, Y)\n",
    "        sigma_xy[1,0] = 0.3 # corr(X2, Y) -> 0.5^2 + 0.3^2 = 0.25 + 0.09 = 0.34 < 1\n",
    "    elif n_feat_x_analytical == 3:\n",
    "        sigma_xy[0,0] = 0.4\n",
    "        sigma_xy[1,0] = 0.3\n",
    "        sigma_xy[2,0] = 0.2 # 0.16 + 0.09 + 0.04 = 0.29 < 1\n",
    "    else: # Default para mais features\n",
    "         for i in range(n_feat_x_analytical):\n",
    "            sigma_xy[i,0] = (target_overall_correlation_strength / np.sqrt(n_feat_x_analytical)) * (0.8 + 0.4*np.random.rand()) # Adiciona alguma variação\n",
    "\n",
    "\n",
    "    # Montar a matriz de covariância completa Sigma\n",
    "    # Sigma = [[Sigma_XX, Sigma_XY],\n",
    "    #          [Sigma_YX, Sigma_YY]]\n",
    "    sigma_yx = sigma_xy.T\n",
    "    top_block = np.hstack((sigma_xx, sigma_xy))\n",
    "    bottom_block = np.hstack((sigma_yx, sigma_yy))\n",
    "    cov_matrix_full_gaussian = np.vstack((top_block, bottom_block))\n",
    "\n",
    "    # Checar se é positiva definida (Cholesky falhará se não for)\n",
    "    try:\n",
    "        np.linalg.cholesky(cov_matrix_full_gaussian)\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(f\"AVISO: Matriz de covariância não é positiva definida para N_Features_X = {n_feat_x_analytical}. Pulando este caso.\")\n",
    "        # Pode ser necessário ajustar os valores em sigma_xy se isso acontecer\n",
    "        # Exemplo alternativo mais robusto para Sigma_XY que garante SPD:\n",
    "        # A = np.random.rand(total_dims, total_dims)\n",
    "        # cov_matrix_full_gaussian = np.dot(A, A.transpose()) # Garante SPD\n",
    "        # # E então normalizar para ter variâncias unitárias nas diagonais se desejado,\n",
    "        # # e extrair/definir as correlações. Isso é mais complexo.\n",
    "        # # Por ora, a construção manual de sigma_xy acima deve funcionar para os valores dados.\n",
    "        continue\n",
    "\n",
    "\n",
    "    mean_full_gaussian = np.zeros(total_dims)\n",
    "    data_full_gaussian = np.random.multivariate_normal(mean_full_gaussian, cov_matrix_full_gaussian, n_samples_gaussian_analytical)\n",
    "\n",
    "    X_gaussian_analytical = data_full_gaussian[:, :n_feat_x_analytical]\n",
    "    Y_gaussian_analytical = data_full_gaussian[:, n_feat_x_analytical] # Y é a última coluna, 1D\n",
    "    Y_gaussian_analytical_reshaped = Y_gaussian_analytical.reshape(-1, 1) # Para NPEET\n",
    "\n",
    "    # Calcular MI Teórica I(X;Y)\n",
    "    det_sigma_xx = np.linalg.det(sigma_xx)\n",
    "    det_sigma_yy = sigma_yy_val # Já que Y é 1D, |Sigma_YY| = Var(Y)\n",
    "    det_sigma_full = np.linalg.det(cov_matrix_full_gaussian)\n",
    "\n",
    "    mi_theoretical_gaussian = np.nan\n",
    "    if det_sigma_xx > 1e-9 and det_sigma_yy > 1e-9 and det_sigma_full > 1e-9: # Evitar log de <=0 ou divisão por zero\n",
    "        # Garantir que o argumento do log seja > 0\n",
    "        log_arg = (det_sigma_xx * det_sigma_yy) / det_sigma_full\n",
    "        if log_arg > 1e-9:\n",
    "             mi_theoretical_gaussian = 0.5 * np.log(log_arg)\n",
    "        else:\n",
    "            print(f\"AVISO: Argumento do log para MI teórica <=0 ({log_arg}). MI será NaN.\")\n",
    "    else:\n",
    "        print(f\"AVISO: Determinante zero ou próximo de zero. MI teórica será NaN.\")\n",
    "\n",
    "\n",
    "    print(f\"  MI Teórica (Gaussiana, X {n_feat_x_analytical}-dim): {mi_theoretical_gaussian:.4f} nats\")\n",
    "    param_string_theoretical = f\"X_dims={n_feat_x_analytical}, Y_dims=1, CovXY approx {np.mean(np.abs(sigma_xy)):.2f}\"\n",
    "\n",
    "\n",
    "    overall_results_list.append({\n",
    "        \"Method\": \"Theoretical Gaussian\",\n",
    "        \"N_Features_X\": n_feat_x_analytical,\n",
    "        \"MI (nats, mean)\": mi_theoretical_gaussian,\n",
    "        \"Parameters\": param_string_theoretical,\n",
    "        \"Time (s)\": 0.0\n",
    "    })\n",
    "\n",
    "    # --- 1. Scikit-learn com Dados Gaussianos ---\n",
    "    # Nota: mutual_info_regression retorna MI(X_i; Y) para cada feature X_i.\n",
    "    # Vamos calcular a média desses valores. Não é I(X_vec; Y).\n",
    "    for k_sklearn in k_values_knn:\n",
    "        for n_jobs_sklearn in n_jobs_values_sklearn:\n",
    "            params_sklearn = f\"k={k_sklearn}, n_jobs={n_jobs_sklearn} (Gaussian Test)\"\n",
    "            start_time = time.time()\n",
    "            mi_sklearn_per_feature = mutual_info_regression(X_gaussian_analytical, Y_gaussian_analytical, n_neighbors=k_sklearn, n_jobs=n_jobs_sklearn, random_state=0)\n",
    "            mi_sklearn_val = np.mean(mi_sklearn_per_feature) # Média das MIs feature-wise\n",
    "            end_time = time.time()\n",
    "            time_sklearn = end_time - start_time\n",
    "            overall_results_list.append({\n",
    "                \"Method\": f\"Scikit-learn (n_jobs={n_jobs_sklearn})\",\n",
    "                \"N_Features_X\": n_feat_x_analytical,\n",
    "                \"MI (nats, mean)\": mi_sklearn_val,\n",
    "                \"Parameters\": params_sklearn,\n",
    "                \"Time (s)\": time_sklearn\n",
    "            })\n",
    "\n",
    "    # --- 2. NPEET com Dados Gaussianos ---\n",
    "    # Estima I(X_vec; Y_vec)\n",
    "    for k_npeet in k_values_knn:\n",
    "        params_npeet = f\"k={k_npeet} (Gaussian Test)\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            # X_gaussian_analytical já é (samples, n_feat_x_analytical)\n",
    "            # Y_gaussian_analytical_reshaped é (samples, 1)\n",
    "            mi_npeet_val = ee.mi(X_gaussian_analytical, Y_gaussian_analytical_reshaped, k=k_npeet, base=base_log_manual)\n",
    "        except Exception as e:\n",
    "            print(f\"  Erro no NPEET com dados Gaussianos (X {n_feat_x_analytical}-dim): {e}\")\n",
    "            mi_npeet_val = np.nan\n",
    "        end_time = time.time()\n",
    "        time_npeet = end_time - start_time\n",
    "        overall_results_list.append({\n",
    "            \"Method\": \"NPEET\",\n",
    "            \"N_Features_X\": n_feat_x_analytical,\n",
    "            \"MI (nats, mean)\": mi_npeet_val,\n",
    "            \"Parameters\": params_npeet,\n",
    "            \"Time (s)\": time_npeet\n",
    "        })\n",
    "    \n",
    "    # --- 3. Binarização com Dados Gaussianos ---\n",
    "    def discretizar_variavel_continua(variavel_continua, num_bins):\n",
    "        if len(variavel_continua) == 0: return np.array([])\n",
    "        min_val = np.min(variavel_continua)\n",
    "        max_val = np.max(variavel_continua)\n",
    "        if min_val == max_val: return np.zeros(len(variavel_continua), dtype=int)\n",
    "        bins = np.linspace(min_val, max_val + 1e-9 * abs(max_val) if max_val != 0 else 1e-9, num_bins + 1)\n",
    "        variavel_discretizada = np.digitize(variavel_continua, bins=bins) - 1\n",
    "        return np.clip(variavel_discretizada, 0, num_bins - 1)\n",
    "\n",
    "    def calcular_informacao_mutua_binning_1d(X_1d_continuo, Y_1d_continuo, num_bins_x, num_bins_y, base_log=base_log_manual):\n",
    "        N = len(X_1d_continuo)\n",
    "        if N == 0: return 0.0\n",
    "        X_binned = discretizar_variavel_continua(X_1d_continuo, num_bins_x)\n",
    "        Y_binned = discretizar_variavel_continua(Y_1d_continuo, num_bins_y)\n",
    "        p_xy = defaultdict(float)\n",
    "        for i in range(N): p_xy[(X_binned[i], Y_binned[i])] += 1.0\n",
    "        for key in p_xy: p_xy[key] /= N\n",
    "        p_x = Counter(X_binned); p_y = Counter(Y_binned)\n",
    "        for key in p_x: p_x[key] /= N\n",
    "        for key in p_y: p_y[key] /= N\n",
    "        mutual_info = 0.0\n",
    "        for (x_val, y_val), joint_prob in p_xy.items():\n",
    "            if joint_prob > 1e-12: \n",
    "                marginal_x_prob = p_x.get(x_val, 0)\n",
    "                marginal_y_prob = p_y.get(y_val, 0)\n",
    "                if marginal_x_prob > 1e-12 and marginal_y_prob > 1e-12: \n",
    "                    mutual_info += joint_prob * np.log(joint_prob / (marginal_x_prob * marginal_y_prob))\n",
    "        if base_log != np.e: mutual_info /= np.log(base_log)\n",
    "        return mutual_info\n",
    "\n",
    "    for num_bins_current in n_bins_values_binning:\n",
    "        params_binning = f\"bins={num_bins_current} (Gaussian Test)\"\n",
    "        mi_binning_feature_wise_gaussian = []\n",
    "        time_binning_total_gaussian = 0\n",
    "        for j in range(n_feat_x_analytical):\n",
    "            start_time = time.time()\n",
    "            mi_j = calcular_informacao_mutua_binning_1d(X_gaussian_analytical[:, j], Y_gaussian_analytical, num_bins_current, num_bins_current, base_log=base_log_manual)\n",
    "            end_time = time.time()\n",
    "            time_binning_total_gaussian += (end_time - start_time)\n",
    "            mi_binning_feature_wise_gaussian.append(mi_j)\n",
    "        \n",
    "        mi_binning_val = np.mean(mi_binning_feature_wise_gaussian) if mi_binning_feature_wise_gaussian else np.nan\n",
    "        \n",
    "        overall_results_list.append({\n",
    "            \"Method\": \"Binarização\",\n",
    "            \"N_Features_X\": n_feat_x_analytical,\n",
    "            \"MI (nats, mean)\": mi_binning_val,\n",
    "            \"Parameters\": params_binning,\n",
    "            \"Time (s)\": time_binning_total_gaussian\n",
    "        })\n",
    "\n",
    "    # --- 4. KDE com Dados Gaussianos ---\n",
    "    # Estima I(X_vec; Y)\n",
    "    epsilon_kde = 1e-10\n",
    "    params_kde = \"gaussian_kde default bw (Gaussian Test)\"\n",
    "    start_time = time.time()\n",
    "    mi_kde_val = np.nan\n",
    "    h_X_kde, h_Y_kde, h_XY_kde = np.nan, np.nan, np.nan\n",
    "    try:\n",
    "        # H(X_vec) - KDE espera (dims, samples)\n",
    "        kde_X = gaussian_kde(X_gaussian_analytical.T)\n",
    "        log_p_X = np.log(kde_X.pdf(X_gaussian_analytical.T) + epsilon_kde)\n",
    "        h_X_kde = -np.mean(log_p_X)\n",
    "        \n",
    "        # H(Y)\n",
    "        kde_Y = gaussian_kde(Y_gaussian_analytical) # Y é 1D\n",
    "        log_p_Y = np.log(kde_Y.pdf(Y_gaussian_analytical) + epsilon_kde)\n",
    "        h_Y_kde = -np.mean(log_p_Y)\n",
    "        \n",
    "        # H(X_vec, Y)\n",
    "        # np.vstack espera uma tupla de arrays a serem empilhados verticalmente\n",
    "        # X_gaussian_analytical.T é (n_feat_x_analytical, n_samples)\n",
    "        # Y_gaussian_analytical (1D) precisa ser (1, n_samples) para vstack\n",
    "        data_XY = np.vstack((X_gaussian_analytical.T, Y_gaussian_analytical.reshape(1, -1)))\n",
    "        kde_XY = gaussian_kde(data_XY)\n",
    "        log_p_XY = np.log(kde_XY.pdf(data_XY) + epsilon_kde)\n",
    "        h_XY_kde = -np.mean(log_p_XY)\n",
    "        \n",
    "        mi_kde_val = h_X_kde + h_Y_kde - h_XY_kde\n",
    "    except Exception as e:\n",
    "        print(f\"  Erro no KDE com dados Gaussianos (X {n_feat_x_analytical}-dim): {e}\")\n",
    "        mi_kde_val = np.nan\n",
    "        # Se falhar, o tempo total não reflete o cálculo completo\n",
    "        # No entanto, time.time() - start_time ainda dará o tempo até a falha.\n",
    "        # Poderíamos definir time_kde = np.nan também, mas por ora deixamos o tempo parcial.\n",
    "    end_time = time.time()\n",
    "    time_kde = end_time - start_time\n",
    "    overall_results_list.append({\n",
    "        \"Method\": \"KDE\",\n",
    "        \"N_Features_X\": n_feat_x_analytical,\n",
    "        \"MI (nats, mean)\": mi_kde_val,\n",
    "        \"Parameters\": params_kde,\n",
    "        \"Time (s)\": time_kde\n",
    "    })\n",
    "\n",
    "print(\"--- FIM DO TESTE ANALÍTICO COM DADOS GAUSSIANOS ---\")\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "# LOOP ORIGINAL PARA TESTAR COM DIFERENTES NÚMEROS DE FEATURES (DADOS UNIFORMES)\n",
    "#==============================================================================\n",
    "if n_features_to_test_uniform: # Apenas executa se a lista não estiver vazia\n",
    "    print(\"\\n--- INICIANDO TESTES COM DADOS UNIFORMES E COMBINAÇÃO LINEAR ---\")\n",
    "    for n_features_X in n_features_to_test_uniform: # Renomeado para clareza\n",
    "        # --- Gerar Dados Sintéticos ---\n",
    "        X = np.random.uniform(0, 10, (n_samples_global, n_features_X))\n",
    "        if n_features_X > 0:\n",
    "            weights = np.random.rand(n_features_X)\n",
    "            Y = np.dot(X, weights) + np.random.uniform(-1, 1, n_samples_global) * noise_scale_global\n",
    "        else:\n",
    "            X = np.zeros((n_samples_global, 1))\n",
    "            Y = np.random.uniform(-1, 1, n_samples_global) * noise_scale_global\n",
    "        Y_reshaped = Y.reshape(-1, 1)\n",
    "\n",
    "        print(f\"--- Geração de Dados ({n_features_X} features) ---\")\n",
    "        print(f\"Número de amostras: {n_samples_global}\")\n",
    "        print(f\"Escala do ruído: {noise_scale_global:.1f}\")\n",
    "\n",
    "        # --- 1. Cálculo de MI com Scikit-learn ---\n",
    "        for k_sklearn in k_values_knn:\n",
    "            for n_jobs_sklearn in n_jobs_values_sklearn:\n",
    "                params_sklearn = f\"k={k_sklearn}, n_jobs={n_jobs_sklearn}\"\n",
    "                start_time = time.time()\n",
    "                if n_features_X > 0:\n",
    "                    mi_sklearn_values_per_feature = mutual_info_regression(X, Y, n_neighbors=k_sklearn, n_jobs=n_jobs_sklearn, random_state=0)\n",
    "                    mi_sklearn_mean = np.mean(mi_sklearn_values_per_feature)\n",
    "                else:\n",
    "                    mi_sklearn_mean = np.nan\n",
    "                end_time = time.time()\n",
    "                time_sklearn = end_time - start_time\n",
    "                overall_results_list.append({\n",
    "                    \"Method\": f\"Scikit-learn (n_jobs={n_jobs_sklearn})\",\n",
    "                    \"N_Features_X\": n_features_X,\n",
    "                    \"MI (nats, mean)\": mi_sklearn_mean,\n",
    "                    \"Parameters\": params_sklearn,\n",
    "                    \"Time (s)\": time_sklearn\n",
    "                })\n",
    "\n",
    "        # --- 2. Cálculo de MI com NPEET (feature-wise para dados uniformes) ---\n",
    "        # No loop de dados uniformes, n_features_X refere-se às features de X.\n",
    "        # Se n_features_X > 1, calculamos a média de I(X_i; Y).\n",
    "        # Se n_features_X = 1, é I(X;Y).\n",
    "        # Para NPEET, vamos manter a lógica de média feature-wise para consistência com\n",
    "        # Scikit-learn e Binarização neste loop de dados uniformes.\n",
    "        # Se quiséssemos I(X_vec; Y) para dados uniformes com NPEET, chamaríamos uma vez.\n",
    "        \n",
    "        # DECISÃO: Para o loop de dados uniformes, manter a lógica feature-wise para todos\n",
    "        # os métodos que o suportam, para comparar \"apples-to-apples\" o que cada um faz\n",
    "        # quando X tem múltiplas features e Y é 1D. O teste Gaussiano acima\n",
    "        # é onde NPEET e KDE estimam o I(X_vec, Y) teórico.\n",
    "\n",
    "        for k_npeet in k_values_knn:\n",
    "            params_npeet = f\"k={k_npeet}\"\n",
    "            mi_npeet_values_per_feature = []\n",
    "            time_npeet_total = 0\n",
    "            try:\n",
    "                if n_features_X == 0:\n",
    "                     mi_npeet_values_per_feature.append(np.nan)\n",
    "                else:\n",
    "                    for j in range(n_features_X): # Feature-wise\n",
    "                        X_j_reshaped = X[:, j].reshape(-1, 1)\n",
    "                        start_time_feat = time.time()\n",
    "                        mi_j = ee.mi(X_j_reshaped, Y_reshaped, k=k_npeet, base=base_log_manual)\n",
    "                        end_time_feat = time.time()\n",
    "                        time_npeet_total += (end_time_feat - start_time_feat)\n",
    "                        mi_npeet_values_per_feature.append(mi_j)\n",
    "                if mi_npeet_values_per_feature and not all(np.isnan(mi_npeet_values_per_feature)):\n",
    "                    mi_npeet_mean = np.mean(mi_npeet_values_per_feature)\n",
    "                else:\n",
    "                    mi_npeet_mean = np.nan\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao calcular MI com NPEET para uma feature (dados uniformes): {e}\")\n",
    "                mi_npeet_mean = np.nan\n",
    "                time_npeet_total = np.nan\n",
    "            overall_results_list.append({\n",
    "                \"Method\": \"NPEET\",\n",
    "                \"N_Features_X\": n_features_X,\n",
    "                \"MI (nats, mean)\": mi_npeet_mean,\n",
    "                \"Parameters\": params_npeet,\n",
    "                \"Time (s)\": time_npeet_total\n",
    "            })\n",
    "\n",
    "        # --- 3. Cálculo de MI com Método de Binarização (Binning) (feature-wise) ---\n",
    "        for num_bins_current in n_bins_values_binning:\n",
    "            params_binning = f\"bins={num_bins_current}\"\n",
    "            mi_binning_values_per_feature = []\n",
    "            time_binning_total = 0\n",
    "            if n_features_X == 0:\n",
    "                 mi_binning_values_per_feature.append(np.nan)\n",
    "            else:\n",
    "                for j in range(n_features_X): # Feature-wise\n",
    "                    start_time_feat = time.time()\n",
    "                    mi_j = calcular_informacao_mutua_binning_1d(X[:, j], Y, num_bins_current, num_bins_current, base_log=base_log_manual)\n",
    "                    end_time_feat = time.time()\n",
    "                    time_binning_total += (end_time_feat - start_time_feat)\n",
    "                    mi_binning_values_per_feature.append(mi_j)\n",
    "            if mi_binning_values_per_feature and not all(np.isnan(mi_binning_values_per_feature)):\n",
    "                mi_binning_mean = np.mean(mi_binning_values_per_feature)\n",
    "            else:\n",
    "                mi_binning_mean = np.nan\n",
    "            overall_results_list.append({\n",
    "                \"Method\": \"Binarização\",\n",
    "                \"N_Features_X\": n_features_X,\n",
    "                \"MI (nats, mean)\": mi_binning_mean,\n",
    "                \"Parameters\": params_binning,\n",
    "                \"Time (s)\": time_binning_total\n",
    "            })\n",
    "\n",
    "        # --- 4. Cálculo de MI com Estimativa de Densidade por Kernel (KDE) (feature-wise) ---\n",
    "        epsilon_kde = 1e-10\n",
    "        params_kde = \"gaussian_kde default bw\"\n",
    "        mi_kde_values_per_feature = []\n",
    "        time_kde_total = 0\n",
    "        kde_calculation_successful_for_at_least_one_feature = False\n",
    "        h_Y_kde_val = np.nan\n",
    "        try:\n",
    "            kde_Y_global = gaussian_kde(Y)\n",
    "            log_p_Y_global = np.log(kde_Y_global.pdf(Y) + epsilon_kde)\n",
    "            h_Y_kde_val = -np.mean(log_p_Y_global)\n",
    "        except Exception as e_hy:\n",
    "            print(f\"Erro ao calcular H(Y) com KDE (dados uniformes): {e_hy}\")\n",
    "\n",
    "        if not np.isnan(h_Y_kde_val):\n",
    "            if n_features_X == 0:\n",
    "                pass\n",
    "            else:\n",
    "                for j in range(n_features_X): # Feature-wise\n",
    "                    X_j = X[:, j]\n",
    "                    h_Xj_kde, h_XjY_kde, mi_j_kde = np.nan, np.nan, np.nan\n",
    "                    try:\n",
    "                        start_time_feature = time.time()\n",
    "                        kde_Xj = gaussian_kde(X_j)\n",
    "                        log_p_Xj = np.log(kde_Xj.pdf(X_j) + epsilon_kde)\n",
    "                        h_Xj_kde = -np.mean(log_p_Xj)\n",
    "                        data_XjY = np.vstack([X_j, Y])\n",
    "                        kde_XjY = gaussian_kde(data_XjY)\n",
    "                        log_p_XjY = np.log(kde_XjY.pdf(data_XjY) + epsilon_kde)\n",
    "                        h_XjY_kde = -np.mean(log_p_XjY)\n",
    "                        mi_j_kde = h_Xj_kde + h_Y_kde_val - h_XjY_kde\n",
    "                        end_time_feature = time.time()\n",
    "                        time_kde_total += (end_time_feature - start_time_feature)\n",
    "                        mi_kde_values_per_feature.append(mi_j_kde)\n",
    "                        kde_calculation_successful_for_at_least_one_feature = True\n",
    "                    except Exception as e_feature:\n",
    "                        print(f\"Erro ao calcular MI com KDE para feature {j} (dados uniformes): {e_feature}\")\n",
    "                        mi_kde_values_per_feature.append(np.nan)\n",
    "        if kde_calculation_successful_for_at_least_one_feature and mi_kde_values_per_feature:\n",
    "            mi_kde_mean = np.nanmean(mi_kde_values_per_feature)\n",
    "        else:\n",
    "            mi_kde_mean = np.nan\n",
    "            time_kde_total = np.nan\n",
    "        overall_results_list.append({\n",
    "            \"Method\": \"KDE\",\n",
    "            \"N_Features_X\": n_features_X,\n",
    "            \"MI (nats, mean)\": mi_kde_mean,\n",
    "            \"Parameters\": params_kde,\n",
    "            \"Time (s)\": time_kde_total\n",
    "        })\n",
    "else:\n",
    "    print(\"\\nNenhum teste com dados uniformes será executado pois 'n_features_to_test_uniform' está vazio.\")\n",
    "\n",
    "\n",
    "# --- Criar e Mostrar DataFrame com os Resultados Finais ---\n",
    "df_results = pd.DataFrame(overall_results_list)\n",
    "\n",
    "if not df_results.empty:\n",
    "    cols_order = [\"Method\", \"N_Features_X\", \"MI (nats, mean)\", \"Parameters\", \"Time (s)\"]\n",
    "    existing_cols_order = [col for col in cols_order if col in df_results.columns]\n",
    "    df_results = df_results[existing_cols_order]\n",
    "\n",
    "    # Lidar com NaNs na coluna 'Time (s)' antes de rankear para evitar erros com alguns tipos de NaN\n",
    "    df_results['Time (s)'] = pd.to_numeric(df_results['Time (s)'], errors='coerce')\n",
    "\n",
    "\n",
    "    df_results['Processing_Time_Rank'] = df_results.groupby('N_Features_X')['Time (s)'] \\\n",
    "                                                 .rank(method='dense', na_option='bottom')\n",
    "    \n",
    "    df_results_ranked_sorted = df_results.sort_values(by=['N_Features_X', 'MI (nats, mean)', 'Processing_Time_Rank'], ascending=[True, False, True]) # Ordenar por N_Features, depois por MI (decrescente), depois por Rank (crescente)\n",
    "    \n",
    "    print(\"\\n--- Resultados Rankeados (Dentro de Cada N_Features_X, Ordenado por MI e Tempo) ---\")\n",
    "    pd.set_option('display.max_rows', None) \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000) \n",
    "\n",
    "    for n_feat, group_df in df_results_ranked_sorted.groupby('N_Features_X'):\n",
    "        print(f\"\\n>>> Resultados para N_Features_X = {n_feat} <<<\")\n",
    "        # Mostrar colunas relevantes, incluindo o MI e o Rank\n",
    "        print(group_df[['Method', 'Parameters', 'MI (nats, mean)', 'Time (s)', 'Processing_Time_Rank']].to_string(index=False))\n",
    "else:\n",
    "    print(\"Nenhum resultado para exibir.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
